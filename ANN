import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import joblib
import seaborn as sns
from sklearn.metrics import ConfusionMatrixDisplay

# Load datasets
train_df = pd.read_csv('/content/drive/MyDrive/DataSets/fraudTrain.csv')
test_df = pd.read_csv('/content/drive/MyDrive/DataSets/fraudTest.csv')

# Drop unnecessary columns
cols_to_drop = ['Unnamed: 0', 'cc_num', 'first', 'last', 'street', 'trans_num']
train_df = train_df.drop(columns=cols_to_drop)
test_df = test_df.drop(columns=cols_to_drop)

# Convert 'trans_date_trans_time' to datetime and extract features
for df in [train_df, test_df]:
    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])
    df['hour'] = df['trans_date_trans_time'].dt.hour
    df['day'] = df['trans_date_trans_time'].dt.day
    df['month'] = df['trans_date_trans_time'].dt.month
    df['year'] = df['trans_date_trans_time'].dt.year
    df.drop(columns=['trans_date_trans_time'], inplace=True)

# Convert 'dob' to age
for df in [train_df, test_df]:
    df['dob'] = pd.to_datetime(df['dob'], errors='coerce')  # Ensure valid datetime format
    df['age'] = df['dob'].apply(lambda x: 2024 - x.year if pd.notnull(x) else np.nan)
    df.drop(columns=['dob'], inplace=True)  # Drop original 'dob' column

# Handle 'gender' (binary categorical variable)
if 'gender' in train_df.columns:
    gender_mapping = {'M': 0, 'F': 1}
    train_df['gender'] = train_df['gender'].map(gender_mapping)
    test_df['gender'] = test_df['gender'].map(gender_mapping)

# One-hot encode 'city' and 'state'
categorical_columns = ['merchant', 'category', 'job', 'city', 'state']
for col in categorical_columns:
    combined_categories = pd.concat([train_df[col], test_df[col]]).drop_duplicates().reset_index(drop=True)
    category_mapping = {cat: idx for idx, cat in enumerate(combined_categories)}
    
    train_df[col] = train_df[col].map(category_mapping)
    test_df[col] = test_df[col].map(category_mapping)

# Fill missing values after mapping unseen categories
train_df.fillna(-1, inplace=True)
test_df.fillna(-1, inplace=True)

# Double-check for any remaining non-numeric columns
if not train_df.select_dtypes(include=['object']).empty:
    print("Non-numeric columns in train set:", train_df.select_dtypes(include=['object']).columns)
if not test_df.select_dtypes(include=['object']).empty:
    print("Non-numeric columns in test set:", test_df.select_dtypes(include=['object']).columns)

# Ensure there are no non-numeric columns before training
assert train_df.select_dtypes(include=['object']).empty, "Train set still contains non-numeric columns!"
assert test_df.select_dtypes(include=['object']).empty, "Test set still contains non-numeric columns!"

# Separate features and target variable
X_train = train_df.drop('is_fraud', axis=1)
y_train = train_df['is_fraud']
X_test = test_df.drop('is_fraud', axis=1)
y_test = test_df['is_fraud']

# Train-Test Split for validation
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Normalize features for ANN (scaling to [0,1])
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Initialize and build the ANN model
model = Sequential()

# Add input layer and first hidden layer
model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))

# Add second hidden layer
model.add(Dense(units=32, activation='relu'))

# Add output layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=64, validation_data=(X_val_scaled, y_val))

# Make predictions
y_val_pred = (model.predict(X_val_scaled) > 0.5).astype("int32")
y_test_pred = (model.predict(X_test_scaled) > 0.5).astype("int32")

# Evaluate the model
print("Validation Classification Report:")
print(classification_report(y_val, y_val_pred))
print("Validation Confusion Matrix:")
print(confusion_matrix(y_val, y_val_pred))
print("Validation ROC-AUC Score:", roc_auc_score(y_val, model.predict(X_val_scaled)))

print("\nTest Classification Report:")
print(classification_report(y_test, y_test_pred))
print("Test Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))
print("Test ROC-AUC Score:", roc_auc_score(y_test, model.predict(X_test_scaled)))

# Save the model (optional)
model.save('fraud_detection_ann_model.h5')

# Save the scaler for future use
joblib.dump(scaler, 'scaler.pkl')

# Visualization of Training History (Loss and Accuracy)
plt.figure(figsize=(12, 6))

# Plot Training & Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot Training & Validation Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# ROC Curve for validation and test sets
fpr_val, tpr_val, _ = roc_curve(y_val, model.predict(X_val_scaled))
fpr_test, tpr_test, _ = roc_curve(y_test, model.predict(X_test_scaled))

plt.figure(figsize=(10, 6))
plt.plot(fpr_val, tpr_val, label=f'Validation ROC Curve (AUC = {auc(fpr_val, tpr_val):.2f})')
plt.plot(fpr_test, tpr_test, label=f'Test ROC Curve (AUC = {auc(fpr_test, tpr_test):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Precision-Recall Curve
precision_val, recall_val, _ = precision_recall_curve(y_val, model.predict(X_val_scaled))
precision_test, recall_test, _ = precision_recall_curve(y_test, model.predict(X_test_scaled))

plt.figure(figsize=(10, 6))
plt.plot(recall_val, precision_val, label=f'Validation Precision-Recall Curve (AP = {average_precision_score(y_val, model.predict(X_val_scaled)):.2f})')
plt.plot(recall_test, precision_test, label=f'Test Precision-Recall Curve (AP = {average_precision_score(y_test, model.predict(X_test_scaled)):.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.show()

# Confusion Matrix Heatmap for Validation and Test Sets
conf_matrix_val = confusion_matrix(y_val, y_val_pred)
conf_matrix_test = confusion_matrix(y_test, y_test_pred)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# For Validation set
disp_val = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_val, display_labels=['Not Fraud', 'Fraud'])
disp_val.plot(cmap='Blues', ax=axes[0])
axes[0].set_title('Validation Confusion Matrix')

# For Test set
disp_test = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_test, display_labels=['Not Fraud', 'Fraud'])
disp_test.plot(cmap='Blues', ax=axes[1])
axes[1].set_title('Test Confusion Matrix')

plt.tight_layout()
plt.show()
